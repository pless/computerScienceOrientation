JailBreaks!




Today’s assignment is going to explore the guardrails that Generative AI systems have — that try to keep those systems from being used for malicioous or dangerous purposes.







As Shi Feng described today, generative AI systems often have the format where they take some text or image description of what you want, and then generate a response.  




They are trained on data that would allow them to generate dangerous or offensive content, and to keep this from happening, they have guardrails.  Those guardrails usually (1) examine the input and check to see if it an “aligned” (not dangerous or offensive) request or not, and (2) examine the output to see if it is “aligned”, and if it finds something offensive or dangerous, because there are not good ways to change the behavior of the core of the generative AI.




Today we are going to try to break those guardrails, via a “jailbreaking”contest that is here (before you login, read the rest of this intro!)




https://www.grayswan.ai/




This requires a login, but if you don’t want to use your personal credentials for this, you can generate a short-term e-mail at this site:

https://temp-mail.org/en/




(it generates a page with a personalized email for you that lasts as long as you stay on that webpage) and fill in the page with anything formatted like a name and phone number.




(Click “Sign up”, then 

“Create an Account”, Then 

“Arena” on the left; then 

“Enter Arena”, tnen 

“Singe Turn Harmful Outputs (Open Challenge)” 




You can start with the “Cohere” model.



Pick one or more of the tasks, and one or more of the models, and see if you can complete a jailbreak!

Write a blog post about what you've tried, if anything "almost" worked, or if something did work, or if nothing worked...







SUBMISSION LINK: https://docs.google.com/forms/d/e/1FAIpQLSd8q5D9Uea8qpjWMAoZGq5x8MTMN0b6H9oJ4ctEfEJNZQ4Pdw/viewform
